"""
Golden payoff validation tests.

Validates Python payoff implementations against hand-verified truth tables.
This file is critical for Julia cross-validation - any changes to payoff
logic should be reflected in both the truth tables and Julia implementation.

Truth table: tests/references/payoff_truth_tables.csv
Generated by: scripts/generate_payoff_truth_tables.py
"""

import csv
from pathlib import Path

import pytest

from annuity_pricing.options.payoffs.fia import (
    CappedCallPayoff,
    ParticipationPayoff,
    SpreadPayoff,
    TriggerPayoff,
)
from annuity_pricing.options.payoffs.rila import (
    BufferPayoff,
    BufferWithFloorPayoff,
    FloorPayoff,
    StepRateBufferPayoff,
)

TRUTH_TABLE_PATH = Path(__file__).parent.parent / "references" / "payoff_truth_tables.csv"

# Tolerance for floating point comparison
# Using exact comparison (1e-10) since all values are deterministic
TOLERANCE = 1e-10


def load_truth_table() -> list[dict]:
    """Load payoff truth table from CSV."""
    if not TRUTH_TABLE_PATH.exists():
        pytest.skip(f"Truth table not found: {TRUTH_TABLE_PATH}")

    with open(TRUTH_TABLE_PATH) as f:
        reader = csv.DictReader(f)
        return list(reader)


def parse_float(value: str) -> float | None:
    """Parse float from CSV, handling empty strings."""
    if value == "" or value is None:
        return None
    return float(value)


def parse_bool(value: str) -> bool:
    """Parse boolean from CSV."""
    return value.lower() == "true"


def create_payoff_from_row(row: dict):
    """Create appropriate payoff object from truth table row."""
    method = row["method"]
    cap_rate = parse_float(row["cap_rate"])
    participation_rate = parse_float(row["participation_rate"])
    spread_rate = parse_float(row["spread_rate"])
    trigger_rate = parse_float(row["trigger_rate"])
    trigger_threshold = parse_float(row["trigger_threshold"])
    buffer_rate = parse_float(row["buffer_rate"])
    floor_rate = parse_float(row["floor_rate"])

    if method == "cap":
        return CappedCallPayoff(
            cap_rate=cap_rate,
            floor_rate=floor_rate if floor_rate is not None else 0.0,
        )
    elif method == "participation":
        return ParticipationPayoff(
            participation_rate=participation_rate,
            floor_rate=floor_rate if floor_rate is not None else 0.0,
            cap_rate=cap_rate,
        )
    elif method == "spread":
        return SpreadPayoff(
            spread_rate=spread_rate,
            floor_rate=floor_rate if floor_rate is not None else 0.0,
            cap_rate=cap_rate,
        )
    elif method == "trigger":
        return TriggerPayoff(
            trigger_rate=trigger_rate,
            trigger_threshold=trigger_threshold if trigger_threshold is not None else 0.0,
            floor_rate=floor_rate if floor_rate is not None else 0.0,
        )
    elif method == "buffer":
        return BufferPayoff(
            buffer_rate=buffer_rate,
            cap_rate=cap_rate,
            floor_rate=floor_rate,
        )
    elif method == "floor":
        return FloorPayoff(
            floor_rate=floor_rate,
            cap_rate=cap_rate,
        )
    elif method == "buffer_floor":
        return BufferWithFloorPayoff(
            buffer_rate=buffer_rate,
            floor_rate=floor_rate,
            cap_rate=cap_rate,
        )
    elif method == "step_rate":
        # Step rate uses tier1=buffer_rate, tier2=buffer_rate, tier2_protection=0.5
        return StepRateBufferPayoff(
            tier1_buffer=buffer_rate,
            tier2_buffer=buffer_rate,
            tier2_protection=0.50,
            cap_rate=cap_rate,
        )
    elif method == "buffer_vs_floor":
        # Comparison cases use buffer for the expected_payoff
        return BufferPayoff(
            buffer_rate=buffer_rate,
            cap_rate=cap_rate,
        )
    else:
        raise ValueError(f"Unknown method: {method}")


# Load truth table once for all tests
TRUTH_TABLE = load_truth_table()


@pytest.mark.parametrize(
    "row",
    TRUTH_TABLE,
    ids=lambda row: row.get("test_id", "unknown"),
)
def test_payoff_matches_truth_table(row: dict):
    """
    Verify Python payoff implementation matches truth table.

    This test is critical for Julia cross-validation.
    Any failure here indicates a mismatch that will propagate to Julia.
    """
    payoff = create_payoff_from_row(row)
    index_return = parse_float(row["index_return"])
    expected_payoff = parse_float(row["expected_payoff"])

    result = payoff.calculate(index_return)

    assert result.credited_return == pytest.approx(
        expected_payoff, abs=TOLERANCE
    ), (
        f"Payoff mismatch for {row['test_id']}: "
        f"expected {expected_payoff}, got {result.credited_return}. "
        f"Edge case: {row['edge_case']}"
    )


class TestFIAAntiPatterns:
    """Anti-pattern tests for FIA payoffs - ensure 0% floor is enforced."""

    @pytest.mark.parametrize(
        "index_return",
        [-0.50, -0.25, -0.10, -0.05, -0.01, -0.001],
    )
    def test_cap_floor_enforcement(self, index_return: float):
        """[T1] FIA cap payoff must never be negative."""
        payoff = CappedCallPayoff(cap_rate=0.10, floor_rate=0.0)
        result = payoff.calculate(index_return)
        assert result.credited_return >= 0.0, f"Negative payoff: {result.credited_return}"
        assert result.floor_applied, "Floor should be applied for negative returns"

    @pytest.mark.parametrize(
        "index_return,participation_rate",
        [
            (-0.20, 0.80),
            (-0.10, 1.50),
            (-0.50, 1.20),
        ],
    )
    def test_participation_floor_enforcement(self, index_return: float, participation_rate: float):
        """[T1] FIA participation payoff must never be negative."""
        payoff = ParticipationPayoff(participation_rate=participation_rate, floor_rate=0.0)
        result = payoff.calculate(index_return)
        assert result.credited_return >= 0.0, f"Negative payoff: {result.credited_return}"

    @pytest.mark.parametrize(
        "index_return,spread_rate",
        [
            (0.01, 0.02),  # spread > return
            (0.02, 0.05),  # spread >> return
            (-0.10, 0.02),  # negative return
        ],
    )
    def test_spread_floor_enforcement(self, index_return: float, spread_rate: float):
        """[T1] FIA spread payoff must never be negative."""
        payoff = SpreadPayoff(spread_rate=spread_rate, floor_rate=0.0)
        result = payoff.calculate(index_return)
        assert result.credited_return >= 0.0, f"Negative payoff: {result.credited_return}"


class TestRILAAntiPatterns:
    """Anti-pattern tests for RILA payoffs."""

    def test_buffer_exact_boundary(self):
        """[T1] At exact buffer boundary, credited return should be 0."""
        payoff = BufferPayoff(buffer_rate=0.10, cap_rate=0.20)
        result = payoff.calculate(-0.10)
        assert result.credited_return == pytest.approx(0.0, abs=TOLERANCE)

    def test_buffer_absorbs_within_range(self):
        """[T1] Buffer should absorb all losses within buffer range."""
        payoff = BufferPayoff(buffer_rate=0.10, cap_rate=0.20)

        for loss in [-0.01, -0.05, -0.08, -0.10]:
            result = payoff.calculate(loss)
            assert result.credited_return == 0.0, f"Loss {loss} should be absorbed"

    def test_100_percent_buffer_edge_case(self):
        """[T1] 100% buffer should provide full loss protection."""
        payoff = BufferPayoff(buffer_rate=1.0, cap_rate=0.25)

        # Even extreme losses should be fully protected
        for loss in [-0.30, -0.50, -0.75, -0.99]:
            result = payoff.calculate(loss)
            assert result.credited_return == 0.0, f"100% buffer should protect {loss}"

    def test_floor_at_boundary(self):
        """[T1] At exact floor boundary, credited return equals floor."""
        payoff = FloorPayoff(floor_rate=-0.10, cap_rate=0.20)
        result = payoff.calculate(-0.10)
        assert result.credited_return == pytest.approx(-0.10, abs=TOLERANCE)

    def test_floor_protects_beyond(self):
        """[T1] Floor should protect all losses beyond floor level."""
        payoff = FloorPayoff(floor_rate=-0.10, cap_rate=0.20)

        for loss in [-0.15, -0.25, -0.50]:
            result = payoff.calculate(loss)
            assert result.credited_return == -0.10, f"Floor should limit loss at {loss}"

    def test_buffer_vs_floor_at_breakeven(self):
        """[T1] Buffer and floor should have equal payoff at specific point."""
        # For 10% buffer vs -10% floor, they're equal at -20% return
        # Buffer: -20% + 10% = -10%
        # Floor: max(-20%, -10%) = -10%
        buffer_payoff = BufferPayoff(buffer_rate=0.10, cap_rate=0.20)
        floor_payoff = FloorPayoff(floor_rate=-0.10, cap_rate=0.20)

        buffer_result = buffer_payoff.calculate(-0.20)
        floor_result = floor_payoff.calculate(-0.20)

        assert buffer_result.credited_return == pytest.approx(
            floor_result.credited_return, abs=TOLERANCE
        )


class TestTruthTableIntegrity:
    """Tests to ensure truth table integrity."""

    def test_truth_table_exists(self):
        """Truth table file should exist."""
        assert TRUTH_TABLE_PATH.exists(), f"Missing: {TRUTH_TABLE_PATH}"

    def test_truth_table_has_records(self):
        """Truth table should have records."""
        assert len(TRUTH_TABLE) > 0, "Truth table is empty"

    def test_truth_table_minimum_coverage(self):
        """Truth table should cover minimum expected cases."""
        # Plan called for ~150 rows, we have 135
        assert len(TRUTH_TABLE) >= 100, f"Expected >= 100 rows, got {len(TRUTH_TABLE)}"

    def test_all_methods_covered(self):
        """All payoff methods should be covered in truth table."""
        methods = {row["method"] for row in TRUTH_TABLE}

        expected_methods = {
            "cap", "participation", "spread", "trigger",  # FIA
            "buffer", "floor", "buffer_floor", "step_rate",  # RILA
            "buffer_vs_floor",  # Comparison
        }

        missing = expected_methods - methods
        assert not missing, f"Missing methods in truth table: {missing}"

    def test_100_percent_buffer_in_table(self):
        """100% buffer edge case should be in truth table."""
        buffer_100_cases = [
            row for row in TRUTH_TABLE
            if row["method"] == "buffer" and parse_float(row.get("buffer_rate", "0")) == 1.0
        ]
        assert len(buffer_100_cases) >= 1, "Missing 100% buffer edge case"


class TestCrossValidationReadiness:
    """Tests to ensure data is ready for Julia cross-validation."""

    def test_csv_format_valid(self):
        """CSV should have consistent column structure."""
        expected_columns = {
            "test_id", "category", "method", "index_return",
            "cap_rate", "participation_rate", "spread_rate",
            "trigger_rate", "trigger_threshold", "buffer_rate", "floor_rate",
            "expected_payoff", "cap_applied", "floor_applied",
            "edge_case", "formula", "reference",
        }

        if TRUTH_TABLE:
            actual_columns = set(TRUTH_TABLE[0].keys())
            missing = expected_columns - actual_columns
            assert not missing, f"Missing columns: {missing}"

    def test_no_null_expected_payoffs(self):
        """All rows should have expected_payoff values."""
        for row in TRUTH_TABLE:
            payoff = parse_float(row.get("expected_payoff"))
            assert payoff is not None, f"Missing expected_payoff in {row.get('test_id')}"

    def test_deterministic_reproducibility(self):
        """
        Verify all truth table values can be reproduced exactly.

        This is critical - Julia implementation should match these exact values.
        """
        mismatches = []

        for row in TRUTH_TABLE:
            try:
                payoff = create_payoff_from_row(row)
                index_return = parse_float(row["index_return"])
                expected = parse_float(row["expected_payoff"])

                result = payoff.calculate(index_return)

                if abs(result.credited_return - expected) > TOLERANCE:
                    mismatches.append({
                        "test_id": row["test_id"],
                        "expected": expected,
                        "actual": result.credited_return,
                    })
            except Exception as e:
                mismatches.append({
                    "test_id": row.get("test_id", "unknown"),
                    "error": str(e),
                })

        assert not mismatches, f"Reproducibility failures: {mismatches[:5]}"
